{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-18T14:29:34.152314400Z",
     "start_time": "2023-05-18T14:29:34.149177500Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import gym  # OpenAI Gym库，用于创建和操作环境\n",
    "import numpy as np  # NumPy库，用于进行数值计算\n",
    "import matplotlib.pyplot as plt # Matplotlib的pyplot，用于绘图\n",
    "import torch as torch\n",
    "\n",
    "import os   # os库，用于处理操作系统相关的任务，如环境变量设置\n",
    "os.environ['SDL_VIDEODRIVER'] = 'dummy' # 设置环境变量，尝试在没有显示设备的情况下运行Gym环境\n",
    "\n",
    "from IPython.display import clear_output    # 用于清除输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')   # 创建环境"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T14:29:34.160841200Z",
     "start_time": "2023-05-18T14:29:34.151313700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset() # 重置环境并获取初始屏幕渲染\n",
    "screen = env.render('rgb_array')    # 使用'rgb_array'模式进行渲染，获取当前图像\n",
    "screen = np.ascontiguousarray(screen,dtype=np.float32)/255. # 将图像数组转换为连续的浮点数数组，并进行归一化\n",
    "\n",
    "plt.figure()    # 绘制当前屏幕图像\n",
    "plt.imshow(screen)  # 使用imshow函数显示图像\n",
    "plt.title('CartPole')   # 设置图像标题\n",
    "plt.xticks([])  # 隐藏x轴刻度\n",
    "plt.yticks([])  # 隐藏y轴刻度\n",
    "plt.show()  # 显示图像"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T14:29:34.205030900Z",
     "start_time": "2023-05-18T14:29:34.155841300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# 定义一个函数，用于绘制结果\n",
    "def plot_result(values, title=''):\n",
    "    clear_output(True)  # 清除当前输出\n",
    "    f, ax = plt.subplots(1,2,figsize=(12,5))    # 创建一个1x2的子图布局，并设置总体尺寸\n",
    "    f.suptitle(title)   # 设置总体标题\n",
    "\n",
    "    # 在第一个子图中，绘制每个episode的奖励\n",
    "    ax[0].plot(values, label='rewards per episode')\n",
    "    ax[0].axhline(200, c='red', label='goal')   # 绘制目标线\n",
    "    ax[0].set_xlabel('Episode') # 设置x轴标签\n",
    "    ax[0].set_ylabel('Reward')  # 设置y轴标签\n",
    "    ax[0].legend()  # 显示图例\n",
    "\n",
    "    # 在第二个子图中，绘制最后50个episode的奖励分布\n",
    "    ax[1].set_title('mean reward = {}'.format(sum(values[-50:])/50))    # 设置标题为平均奖励值\n",
    "    ax[1].hist(values[-50:], label='rewards count')    # 绘制直方图\n",
    "    ax[1].axvline(200,c='red',label='goal') # 绘制目标线\n",
    "    ax[1].set_xlabel('Reward per Last 50 Episodes') # 设置x轴标签\n",
    "    ax[1].set_ylabel('Requency')    # 设置y轴标签\n",
    "    ax[1].legend()  # 显示图例\n",
    "\n",
    "    plt.show()  # 显示图像\n",
    "\n",
    "# 定义一个随机策略函数，执行指定次数的episode\n",
    "def random_policy(env, episodes):\n",
    "    rewards = []    # 创建一个空列表，用于存储每个episode的总奖励\n",
    "    for _ in range(episodes):\n",
    "        env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done :\n",
    "            action = env.action_space.sample()  # 从动作空间中随机选取一个动作\n",
    "            next_state,reward,done,_ = env.step(action) # 执行动作，并获取下一状态、奖励和结束标志\n",
    "            total_reward += reward  # 累加奖励\n",
    "        rewards.append(total_reward)    # 将本episode的总奖励添加到列表中\n",
    "        plot_result(rewards)    # 绘制结果\n",
    "\n",
    "# random_policy(env,50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T14:29:34.209460200Z",
     "start_time": "2023-05-18T14:29:34.206028400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    # 初始化函数，定义了网络结构，损失函数以及优化器\n",
    "    def __init__(self, state_dim:int, action_dim:int, hidden_dim=64, lr=0.001): #状态空间大小，动作空间大小，隐藏层神经网络大小，学习率lr\n",
    "        self.model = torch.nn.Sequential(   # 创建神经网络，包含两个隐藏层和LeakyReLU激活函数\n",
    "            torch.nn.Linear(state_dim,hidden_dim),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(hidden_dim,hidden_dim*2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(hidden_dim*2,action_dim)\n",
    "        )\n",
    "        self.criterion = torch.nn.MSELoss() # 定义损失函数为均方误差损失\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=lr)    # 定义优化器为Adam，学习率为lr\n",
    "\n",
    "    # 更新网络的函数，输入为状态和Q值，计算损失并反向传播\n",
    "    def update(self, state:np.ndarray, q:np.ndarray):\n",
    "        # 预测Q值\n",
    "        q_pred = self.model(torch.Tensor(state))\n",
    "        # 计算预测Q值和实际Q值的均方误差损失\n",
    "        loss = self.criterion(q_pred, torch.Tensor(q))\n",
    "        # 清零梯度\n",
    "        self.optimizer.zero_grad()\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 更新网络参数\n",
    "        self.optimizer.step()\n",
    "\n",
    "    # 预测函数，输入为状态，输出为预测的Q值\n",
    "    def predict(self, state:np.ndarray)-> torch.Tensor:\n",
    "        # torch.no_grad()表示不需要计算梯度，不会进行反向传播\n",
    "        with torch.no_grad():\n",
    "            # 返回预测的Q值\n",
    "            return self.model(torch.Tensor(state))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T14:29:34.213457300Z",
     "start_time": "2023-05-18T14:29:34.210457400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# 简单的DQN训练函数, 训练环境env，训练轮数episodes，衰减因子gamma为0.9，随机概率epsilon，衰减率epsilon_decay\n",
    "def dqn_simple(env, model, episodes, gamma=0.9, epsilon=0.3, epsilon_decay=0.9, title='dqn_simple'):\n",
    "    # 初始化奖励列表\n",
    "    rewards = []\n",
    "    # 对每个episode进行循环\n",
    "    for _ in range(episodes):\n",
    "        # 重置环境，获取初始状态\n",
    "        state = env.reset()\n",
    "        # 初始化总奖励\n",
    "        total_reward = 0.\n",
    "        # 初始化结束标志\n",
    "        done = False\n",
    "        # 当episode没有结束时\n",
    "        while not done:\n",
    "            # epsilon-greedy策略，有epsilon的概率选择随机动作\n",
    "            if(random.random() < epsilon):\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # 否则选择Q值最大的动作\n",
    "                q_values = model.predict(state)\n",
    "                action = torch.argmax(q_values).item()\n",
    "            # 执行动作，获取下一状态，奖励和是否结束，并累加奖励\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # 计算Q值，如果episode结束则只考虑即时奖励，否则考虑即时奖励和下一状态的最大Q值乘以衰减因子gamma\n",
    "            q_values = model.predict(state)\n",
    "            q_values[action] = (\n",
    "                reward + (0 if done else gamma * torch.max(model.predict(next_state)).item())\n",
    "            )\n",
    "            # 根据新的Q值更新网络\n",
    "            model.update(state, q_values)\n",
    "            # 更新状态\n",
    "            state = next_state\n",
    "\n",
    "        # 每轮结束后衰减epsilon，但最小值不低于0.01\n",
    "        epsilon = max(epsilon * epsilon_decay, 0.01)\n",
    "        # 将总奖励加入到奖励列表中\n",
    "        rewards.append(total_reward)\n",
    "        # 绘制奖励曲线\n",
    "        plot_result(rewards)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m simple_dqn \u001B[38;5;241m=\u001B[39m DQN(state_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, action_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, hidden_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# 用simple_dqn模型训练环境env，训练轮数为2000，衰减因子gamma为0.9，epsilon初始值为0.5，epsilon衰减率为0.99\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43mdqn_simple\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msimple_dqn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m150\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.9\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.99\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[17], line 23\u001B[0m, in \u001B[0;36mdqn_simple\u001B[1;34m(env, model, episodes, gamma, epsilon, epsilon_decay, title)\u001B[0m\n\u001B[0;32m     21\u001B[0m     action \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(q_values)\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# 执行动作，获取下一状态，奖励和是否结束，并累加奖励\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m next_state, reward, done, _ \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     24\u001B[0m total_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# 计算Q值，如果episode结束则只考虑即时奖励，否则考虑即时奖励和下一状态的最大Q值乘以衰减因子gamma\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[17], line 23\u001B[0m, in \u001B[0;36mdqn_simple\u001B[1;34m(env, model, episodes, gamma, epsilon, epsilon_decay, title)\u001B[0m\n\u001B[0;32m     21\u001B[0m     action \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(q_values)\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# 执行动作，获取下一状态，奖励和是否结束，并累加奖励\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m next_state, reward, done, _ \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     24\u001B[0m total_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# 计算Q值，如果episode结束则只考虑即时奖励，否则考虑即时奖励和下一状态的最大Q值乘以衰减因子gamma\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Program Files\\JetBrains\\PyCharm 2023.1.1\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_frame.py:880\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[1;34m(self, frame, event, arg)\u001B[0m\n\u001B[0;32m    877\u001B[0m             stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    879\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m plugin_stop:\n\u001B[1;32m--> 880\u001B[0m     stopped_on_plugin \u001B[38;5;241m=\u001B[39m \u001B[43mplugin_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmain_debugger\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep_cmd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    881\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m stop:\n\u001B[0;32m    882\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_line:\n",
      "File \u001B[1;32mD:\\Program Files\\JetBrains\\PyCharm 2023.1.1\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[1;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[0;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[1;32m--> 169\u001B[0m         \u001B[43mmain_debugger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Program Files\\JetBrains\\PyCharm 2023.1.1\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Program Files\\JetBrains\\PyCharm 2023.1.1\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "simple_dqn = DQN(state_dim=4, action_dim=2, hidden_dim=64, lr=0.001)\n",
    "# 用simple_dqn模型训练环境env，训练轮数为2000，衰减因子gamma为0.9，epsilon初始值为0.5，epsilon衰减率为0.99\n",
    "dqn_simple(env=env, model=simple_dqn, episodes=150, gamma=0.9, epsilon=0.5, epsilon_decay=0.99)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## simple DQN\n",
    "\n",
    "$$\n",
    "\\begin{align}\\pi(s) = \\arg\\!\\max_a \\ Q(s, a)\\end{align}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
